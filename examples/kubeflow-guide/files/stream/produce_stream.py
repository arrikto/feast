import json
from time import sleep

import numpy as np
import pandas as pd

from datetime import datetime, timedelta

from kafka import KafkaAdminClient, KafkaProducer
from kafka.admin import NewTopic

def create_driver_locations(drivers, start_date, end_date) -> pd.DataFrame:
    """
    Example df generated by this function:
    | event_timestamp  | driver_id | lat       | lon       |
    |------------------+-----------+-----------+-----------|
    | 2021-03-17 19:31 | 1010      | 37.979161 | 23.783086 |
    | 2021-03-18 19:31 | 1010      | 37.979163 | 23.783089 |
    |                  |  ...      |    ...    |    ...    |
    | 2021-03-23 19:31 | 1001      | 35.491932 | 23.783099 |
    """
    df_daily = pd.DataFrame(
        {
            "event_timestamp": [
                pd.Timestamp(dt, unit="ms", tz="UTC").round("ms")
                for dt in pd.date_range(
                    start=start_date, end=end_date, freq="10min", inclusive="left"
                )
            ]
        }
    )
    df_all_drivers = pd.DataFrame()

    for driver in drivers:
        df_daily_copy = df_daily.copy()
        df_daily_copy["driver_id"] = driver
        df_all_drivers = pd.concat([df_daily_copy, df_all_drivers])

    df_all_drivers.reset_index(drop=True, inplace=True)

    rows = df_all_drivers["event_timestamp"].count()

    df_all_drivers["lat"] = np.random.uniform(low=37.97800, high=37.98000, size=rows).astype(np.float32)
    df_all_drivers["lon"] = np.random.uniform(low=23.78300, high=23.78900, size=rows).astype(np.float32)

    return df_all_drivers

def produce_streaming_data(topic_name, servers, df):
    producer = None
    for i in range(20):
        try:
            print("Instantiating Kafka producer")
            producer = KafkaProducer(
                            client_id="producer",
                            bootstrap_servers=servers,
                            api_version=(0, 10, 1)
                        )
            break
        except Exception as e:
            print(
                f"Trying to instantiate producer with bootstrap servers {servers} with error {e}"
            )
            sleep(5)
            pass

    print("Reading parquet")
    df = df.sort_values(by=["event_timestamp", "driver_id"])
    print("Emitting events")
    iteration = 1
    while True:
        for row in df[
            ["driver_id", "event_timestamp", "lat", "lon"]
        ].to_dict("records"):
            # Make event one more year recent to simulate fresher data
            row["event_timestamp"] = (
                row["event_timestamp"] + pd.Timedelta(weeks=52 * iteration)
            ).strftime("%Y-%m-%d %H:%M:%S")
            producer.send(topic_name, json.dumps(row).encode())
            print(row)
            sleep(30.0)
        iteration += 1

drivers=list(range(1001, 1021))
end_date = datetime.utcnow().replace(microsecond=0, second=0) + timedelta(days=1)
start_date = end_date - timedelta(days=1)

print("Creating streaming data for 24 hours starting now!")
driver_locations_df = create_driver_locations(drivers, start_date, end_date)

servers = "kafka-broker.default.svc.cluster.local:29092"
produce_streaming_data("driver-locations", [servers], driver_locations_df)
